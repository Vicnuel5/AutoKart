{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726719d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9d939",
   "metadata": {},
   "source": [
    "## Combinación de todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtén la lista de archivos en la carpeta 'records'\n",
    "folder_path = 'records'\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Crea una lista para almacenar los DataFrames de cada archivo\n",
    "dataframes = []\n",
    "\n",
    "# Lee cada archivo CSV y añádelo a la lista de DataFrames\n",
    "for file in file_list:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Combina todos los DataFrames en uno solo\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Guarda el DataFrame combinado en un nuevo archivo CSV\n",
    "combined_df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b9cb0",
   "metadata": {},
   "source": [
    "## Dibujado de las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo CSV combinado\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Separa las características (atributos) de las etiquetas de clase\n",
    "y = data['action']  # Clases\n",
    "X = data.drop('action', axis=1)  # Atributos\n",
    "\n",
    "# Aplica PCA para reducir a tres dimensiones\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Crea un DataFrame con los componentes principales y las clases\n",
    "pca_df = pd.DataFrame(X_pca, columns=['Componente1', 'Componente2', 'Componente3'])\n",
    "pca_df['Clase'] = y\n",
    "\n",
    "# Gráfico tridimensional\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Dibuja los puntos en el espacio tridimensional\n",
    "for clase in pca_df['Clase'].unique():\n",
    "    class_data = pca_df[pca_df['Clase'] == clase]\n",
    "    ax.scatter(class_data['Componente1'], class_data['Componente2'], class_data['Componente3'], label=clase)\n",
    "\n",
    "# Etiquetas y título\n",
    "ax.set_xlabel('Componente 1')\n",
    "ax.set_ylabel('Componente 2')\n",
    "ax.set_zlabel('Componente 3')\n",
    "ax.set_title('Distribución de Clases en Espacio Tridimensional (PCA) sin clase NONE')\n",
    "\n",
    "# Leyenda\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46527456",
   "metadata": {},
   "source": [
    "## Limpio el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_filename, output_filename):\n",
    "    \n",
    "    # Carga el archivo CSV \n",
    "    data = pd.read_csv(input_filename)\n",
    "    \n",
    "    # Elimino ciertos parámetros\n",
    "    data = data.drop('ray1', axis=1)\n",
    "    data = data.drop('karty', axis=1)\n",
    "    data = data.drop('time', axis=1)\n",
    "    \n",
    "    ray_columns = ['ray2', 'ray3', 'ray4', 'ray5']\n",
    "    for column in ray_columns:\n",
    "        max_value = data[column].max()  # Encuentra el valor máximo en la columna actual\n",
    "        data[column] = data[column].replace(-1, 10)  # Reemplaza -1 por el valor máximo\n",
    "        \n",
    "    # Guardar el dataset limpio en un nuevo archivo CSV\n",
    "    data.to_csv(output_filename, index=False)\n",
    "    \n",
    "clean_data('dataset.csv', 'dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f175c9",
   "metadata": {},
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ca6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(input_filename, output_filename):\n",
    "    \n",
    "    # Carga del dataset\n",
    "    data = pd.read_csv(input_filename) \n",
    "    \n",
    "    # Seleccionar las columnas a normalizar\n",
    "    columns_to_normalize = data.columns[0:data.columns.get_loc('action')]\n",
    "    \n",
    "    # Aplicar One-Hot Encoding a la columna 'acción'\n",
    "    data = pd.get_dummies(data, columns=['action'])\n",
    "    \n",
    "    # Reescalo\n",
    "    scaler = StandardScaler()\n",
    "    data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
    "    \n",
    "    # Guardar el dataset con One-Hot Encoding en un nuevo archivo CSV\n",
    "    data.to_csv(output_filename, index=False)\n",
    "\n",
    "normalize_data('dataset.csv', 'dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e70128",
   "metadata": {},
   "source": [
    "## Perceptrón Multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e57600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ann2 import forward_propagation as n_forward_propagate\n",
    "from ann2 import backward_propagation as n_backprop\n",
    "from ann2 import cost as n_cost\n",
    "from ann2 import converge as n_converge\n",
    "from ann2 import predict as n_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b7e01",
   "metadata": {},
   "source": [
    "## Cargo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd76003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    # Carga el archivo CSV \n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Encuentra el índice de la columna 'action_ACCELERATE'\n",
    "    idx = data.columns.get_loc('action_ACCELERATE')\n",
    "\n",
    "    # Guarda las columnas hasta 'action_ACCELERATE' (excluyendo 'action_ACCELERATE') en una variable X\n",
    "    X = data.iloc[:, :idx].values\n",
    "\n",
    "    # Guarda las columnas a partir de 'action_ACCELERATE' en otra variable y\n",
    "    y = data.iloc[:, idx:].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = read_data('dataset.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc2c97",
   "metadata": {},
   "source": [
    "## Entreno la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_weights(L_in, L_out):\n",
    "    epsilon_init = 5\n",
    "    return np.random.rand(L_out, L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "# Definir las dimensiones de las capas\n",
    "hidden_layers_size = [24, 48, 24]\n",
    "layers_size = [len(X_train[0])] + hidden_layers_size + [len(y_train[0])]\n",
    "\n",
    "# Inicializo las thetas\n",
    "thetas = [[] for _ in range(len(layers_size) - 1)] \n",
    "for i in range(len(thetas)):\n",
    "    thetas[i] = initialize_random_weights(layers_size[i], layers_size[i + 1])\n",
    "    thetas[i] = np.hstack([np.ones((len(thetas[i]), 1)), thetas[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0863c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.01\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "print(\"Entrenando red...\")\n",
    "J_history, thetas_grad = n_converge(thetas, X_train, y_train, lambda_, alpha, iterations)\n",
    "print(\"Red entrenada\")\n",
    "prediction = n_predict(thetas_grad, X_test)\n",
    "print(np.mean(np.argmax(prediction, axis=1) == y_test) * 100, '%')\n",
    "\n",
    "coefficients = []\n",
    "intercepts = []\n",
    "\n",
    "for theta in thetas_grad:\n",
    "    coefficients.append(theta[:, 1:].T)\n",
    "    intercepts.append(theta[:, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb51a6f",
   "metadata": {},
   "source": [
    "## Perceptrón con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "lambda_ = 0.01\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "sklearn_model = MLPClassifier(hidden_layer_sizes=(24, 48, 24), \n",
    "                              activation='logistic', \n",
    "                              learning_rate_init=alpha, \n",
    "                              alpha=lambda_,\n",
    "                              max_iter=iterations)\n",
    "\n",
    "print(\"Entrenando red...\")\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "print(\"Red entrenada\")\n",
    "prediction = sklearn_model.predict(X_test)\n",
    "print(np.mean(np.argmax(prediction, axis=1) == y_test) * 100, '%')\n",
    "\n",
    "coefficients = sklearn_model.coefs_\n",
    "intercepts = sklearn_model.intercepts_\n",
    "\n",
    "thetas_grad = []\n",
    "\n",
    "for coefficient, intercept in zip(coefficients, intercepts):\n",
    "    intercept = np.array(intercept).reshape(1, -1)\n",
    "    thetas_grad.append(np.vstack([intercept, coefficient]).T) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53600cc2",
   "metadata": {},
   "source": [
    "## Parseo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.txt', 'w') as file:\n",
    "    file.write(f\"num_layers:{len(coefficients) + 1}\\n\")\n",
    "    parameter = 0\n",
    "    for coefficient, intercept in zip(coefficients, intercepts):\n",
    "        file.write(f\"parameter:{parameter}\\n\")\n",
    "        file.write(f\"dims:{[str(elemento) for elemento in list(coefficient.shape)]}\\n\")\n",
    "        file.write(f\"name:coefficient\\n\")\n",
    "        file.write(f\"values:[{', '.join(str(x) for x in coefficient.flatten())}]\\n\")\n",
    "        file.write(f\"parameter:{parameter}\\n\")\n",
    "        file.write(f\"dims:{[str(elemento) for elemento in list(np.array(intercept).reshape(1, -1).shape)]}\\n\")\n",
    "        file.write(f\"name:intercepts\\n\")\n",
    "        file.write(f\"values:[{', '.join(str(x) for x in np.array(intercept).flatten())}]\\n\")\n",
    "        parameter += 1\n",
    "        \n",
    "print('Modelo copiado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7642a",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Inicializar el clasificador KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')  \n",
    "\n",
    "# Entrenar el clasificador\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "prediction = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3ff2b",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Crear el clasificador de árbol de decisión\n",
    "tree_classifier = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Entrenar el modelo\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predecir con el conjunto de prueba\n",
    "prediction = tree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42716924",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, max_depth=30, random_state=42)\n",
    "\n",
    "print(\"Entrenando red...\")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "print(\"Red entrenada\")\n",
    "\n",
    "prediction = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f36d6",
   "metadata": {},
   "source": [
    "## Validación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction, axis=1)\n",
    "num_classes = 4\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred[i]\n",
    "        matrix[true_label][pred_label] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "confusion_matrix = confusion_matrix(y_test, prediction, num_classes)\n",
    "for row in confusion_matrix:\n",
    "    print(row)\n",
    "print()\n",
    "\n",
    "def accuracy(y_true, y_pred, confusion_matrix): \n",
    "    num_classes = len(confusion_matrix)\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            total += confusion_matrix[i][j]\n",
    "            if i == j:\n",
    "                correct += confusion_matrix[i][j]\n",
    "                \n",
    "    return correct / total\n",
    "                \n",
    "print(\"Accuracy:\")\n",
    "print(accuracy(y_test, prediction, confusion_matrix))\n",
    "print()\n",
    "\n",
    "# ¿Tiene sentido el MSE si la salida es una variable categórica?\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    mse = sum((y_true - y_pred) ** 2) / n\n",
    "    return mse\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(mean_squared_error(y_test, prediction))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51480a",
   "metadata": {},
   "source": [
    "## Comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee7d29",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "Hemos elegido 5 vecinos para entrenar ya que con menos vecinos, la red tiene menos precisión, sobreajustándose peor a los valores de enternamiento y adaptándose peor a nuevos problemas; y con muchos más vecinos, la red vuelve a perder precisión ya que al estar sujeta a la \"opinión\" de muchos más vecinos no se entrena correctamente, incapaz de resolver los problemas. Hemos probado a utilizar diferentes funciones para realizar el cálculo de la métrica de la distancia de los puntos con los vecinos y nos hemos quedado con Manhattan porque ha sido la que mayor precisión ha dado.\n",
    "\n",
    "#### Decision Tree\n",
    "\n",
    "Hemos cambiado la profundidad máxima, ya que esta controla la longitud máxima del camino desde la raíz hasta las hojas. Un árbol más profundo puede aprender relaciones más complejas, pero también puede sobreajustarse fácilmente. En nuestro caso hemos visto que a partir de 10 nodos, el árbol empieza a perder precisión debido al overfitting y con menos 3 también baja, ya que el modelo no se entrena correctamente.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "Hemos cambiado el número de estimadores para aumentar la cantidad de árboles en el bosque. Cuantos más árboles, más robusto suele ser el modelo. Cuantos más árboles la precisión del modelo mejora, pero también puede aumentar el tiempo de entrenamiento. También hemos cambiado la profundidad máxima de cada árbol del bosque.\n",
    "\n",
    "\n",
    "\n",
    "Entre estos tres modelos elegiría el random forest ya que es con el que mayor precisión se consigue, por lo que me quedaría con él para entrenar la IA. Sin embargo, si buscara un modelo más rápido (y más fácil de implementar), me quedaría con el KNN, ya que no se queda por detrás en precisión y su entrenamiento es prácticamente inmediato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f2989",
   "metadata": {},
   "source": [
    "## Elección de los parámetros de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34d852",
   "metadata": {},
   "source": [
    "Hemos eliminado la y, ya que el escenario es totalmente, plano, e incluso si no lo fuera, este parámetro sería también irrelevante. Preferiría guardarme la inclinación del coche respecto el eje X.\n",
    "\n",
    "Nos hemos quedado con la x y la z ya que entrenamos el modelo para recorrer un único escenario. Si quisiéramos llevar este modelo a diferentes pistas, quitaríamos estos dos parámetros ya que serían ruido para el correcto entrenamiento del modelo.\n",
    "\n",
    "Hemos quitado el primer rayo, ya que cuando realizamos las grabaciones, dejamos su distancia muy corta, resultando en que el rayo casi nunca cambiaba su valor respecto su máximo, significando ruido para el entrenamiento del modelo.\n",
    "\n",
    "También hemos quitado el tiempo, ya que a la hora de poner el modelo a prueba, tomaba las curvas antes de tiempo, chocándose constantemente contra los muros. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
